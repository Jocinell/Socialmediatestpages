<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TABLE PLATFORM</title>
    <link rel="stylesheet" href="class5&6.css">
</head>
<body>
    <div id="container">
        <h1>JOCINELL TABLE CLASS</h1>
        <hr>
        <img src="images (2).jpg">
        <p>Predicting what someone is about to do next based on their body language comes naturally to humans but not so for computers. When we meet another person, they might greet us with a hello, handshake, or even a fist bump. We may not know which gesture will be used, but we can read the situation and respond appropriately.

            In a new study, Columbia Engineering researchers unveil a computer vision technique for giving machines a more intuitive sense for what will happen next by leveraging higher-level associations between people, animals, and objects.</p>
        <img src="images (3).jpg">
        <div id="content">
            <table>
                <tr>
                    <th>PROGRAMMING LANGUAGE</th>
                </tr>
                <tr>
                    <th>JAVA SCRIPT</th>
                </tr>
                <tr>
                    <th>WEB DEVELOPMENT</th>
                    <td>HTML & CSS</td>
                </tr>
                <tr>
                    <td>Types of programmers</td>
                </tr>
                <tr>
                    <th>FRONT-END DEVLOPER</th>
                </tr>
                <tr>
                    <th>Full stack developer</th>
                    <td>web developer</td>
                </tr>
                <tr>
                    <td>Back-end developer</td>
                </tr>
            </table>
        </div> 
    </div>
      
</body>
</html>